\chapter{Implementation and results}
\label{kap:kap4}

In this chapter, we focus on the implementation of ideas from previous chapter
and the practical results we obtained from measuring their performance. First,
we describe the important parts and decisions in our implementation. Then, we
describe how we benchmarked our implementation and the results we obtained.

\section{New decoding method}

\subsection{Implementation}

\paragraph{SDSL library}

We decided to make our solution a part of the \sdsl library~\citep{gog2014theory}. This
is one of the most matured and versatile libraries implementing succinct data structures.
With more than 30 contributors and almost 2\,000 commits, \sdsl is a heavily tested library
offering various implementations of succinct structures such as wavelet tree, FM-index,
bit vectors, suffix array and many more. It allows to easily use different building blocks
to implement more complex structures, i.e. using different bit vector implementations inside
of the wavelet tree. On top of this, thorough tests and benchmarks were devised, mainly by
\cite{gog2014optimized}. The RRR implementation is provided by templated class \texttt{rrr\_vector}
that uses the on the fly-decoding and enables us to use block size from 3 up to 256. To support
these block sizes, \sdsl implements 128 and 256 bit integers. \sdsl provides template
specialization for block size 15 that uses the table decoding method. To support $\access$,
\texttt{rrr\_vector} supports single bit access operator \texttt{[]} and \texttt{get\_int} method.
To facilitate access it uses the encoding scheme presented on Fig.~\label{obr:RRRFinal} that
consists of the array of fixed size elements that stores classes $C$, array of variable length
elements $O$ that stores offsets corresponding to blocks and third array $P$ that stores pointers
to the array $O$. $P$ stores pointer to the beginning of every superblock. Result of $\rank$ is
precomputed for the beginning of every superblock so at first, binary search along these values
is done. Then linear search for the final result is done inside of the superblock. As a select
can be answered using the simple binary search, the first part of answering $\select$ is to
binary search between precomputed values of $\rank$, then to do the select inside of the superblock.
This demonstrates, how the size of superblock can be used to balance the ratio of space used and the
speed of the $\access$, $\rank$ and $\select$ methods. We however decided not to explore this dimension
of RRR and keep the number of blocks in superblock on default value 32.

We decided to use the 15 bit specialization as an underlying solution for the encoding and decoding of
sub-blocks. We provided specialized implementations for block sizes 31, 63 and 127 that are mostly used
in the practical scenarios. We based our specializations on the general implementation of
\texttt{rrr\_vector} and tried to keep the number of changes as small as possible to clearly see the
effects of our new decoding method. We have been able to only alter the two functions that the \sdsl
uses for encoding and decoding namely \texttt{bin\_to\_nr} and \texttt{nr\_to\_bin}. As we mentioned,
the encoding is less performance critical for most of the applications as encoding is done only once
at the beginning. This is why we focused more on the decoding part of the implementation.

\paragraph{Breaking instance into subproblems}

When implementing our decoding method, we put most of our focus on the process of dividing problem to
sub-problems. There are varios reasons why. The first one is that for smaller blocks 
solving the sub-problems is done using the table approach that is quite fast and can be hardly
made faster as it consists only of one table lookup. The second one is that this part is
inherently blocking us from solving the sub-problems. These sub-problems may be potentially
solved in parallel (by instruction-level parallelism), this part is
though harder to parallelize. Dividing the problem into sub-problems consists of
finding the pair of classes $(c_1, c_2)$ for our block $B$. After this, we need to find the number
of ways how the first and second sub-block may look.
These numbers can be precomputed as for every class $c$, there is at most $b+1$ ways how to
write it in a form $c_1+c_2$ for valid $c_1$ and $c_2$. After obtaining these numbers of combinations
for the first and second sub-block, it is then trivial to compute the parameters for sub-routines 
that decode the sub-blocks and then combine their results to obtain decoded block. We devised 3 different
methods to find the pair $(c_1, c_2)$ for our block $B$. We may precompute how many block there are
with class pair equal to $(0, c), (1, c-1),\ldots , (c, 0)$ and name these numbers $C_{0}\ldots C_{c}$.
Example is on Fig.~\ref{table:class_pairs}.
\begin{figure}
	\centerline{
        \begin{tabular}{c c c c}
            $C_k$	&	$(c_1, c_2)$  &   Block count & Offsets of blocks\\
        \hline
			$C_0$	&	$(0, 2)$	&   \tt 3 	&	0-2\\
			$C_1$	&	$(1, 1)$	&	\tt 9 	&	3-11\\
			$C_2$	&	$(2, 0)$	&	\tt 3	&	12-14\\
        \end{tabular}
	}
	\caption[TODO]{
        Table shows all the class pairs with the number of blocks for $b=6, c=2$.
    }
	\label{table:class_pairs}
\end{figure}
We would like to map the offset of the block to the number $C_i$.
One possible way is to precompute the prefix sums, such that $P_i = \sum_{i=0}^{i} C_i$.
As these prefix sums form an increasing sequence, we can binary search for class
that contains our offset. Although binary search has better time complexity, as the number
of buckets where our offset may land is usually small, linear search outperforms the binary
most of the time. In practice, we found that sequential search using the \texttt{SIMD}
instructions leads to the best results for a smaller block sizes.

\lstset{language=C,caption={Linear search for classes of sub-blocks},label=alg:C}
\begin{lstlisting}
uint32_t get_right_class(uint8_t k, uint32_t nr)  {
	int right_k_from = std::max(k - 15, 0);
	int right_k_to = std::min(k, 15);
	int right_k = right_k_from;
	for (; right_k < right_k_to; ++right_k)  {
		uint32_t curr_index = C[k][right_k+1];
		if (curr_index >= nr)  {
			if (curr_index == nr)
				++right_k;
			break;
		}
	}
	return right_k;
}
\end{lstlisting}

\lstset{language=C,caption={SIMD enhanced linear search for classes of sub-blocks},label=alg:C}
\begin{lstlisting}
uint32_t get_right_class(uint8_t k, uint32_t nr)  {
	int right_k_from = std::max(k - 15, 0);
	int right_k_to = std::min(k, 15);
	__m128i keys = _mm_set1_epi32(nr);
	__m128i vec1 =
		_mm_loadu_si128(reinterpret_cast<__m128i*>(&C[k][0]));
	__m128i vec2 =
		_mm_loadu_si128(reinterpret_cast<__m128i*>(&C[k][4]));
	__m128i vec3 =
		_mm_loadu_si128(reinterpret_cast<__m128i*>(&C[k][8]));
	__m128i vec4 =
		_mm_loadu_si128(reinterpret_cast<__m128i*>(&C[k][12]));

	__m128i cmp1 = _mm_cmpgt_epi32(vec1, keys);
	__m128i cmp2 = _mm_cmpgt_epi32(vec2, keys);
	__m128i cmp3 = _mm_cmpgt_epi32(vec3, keys);
	__m128i cmp4 = _mm_cmpgt_epi32(vec4, keys);

	__m128i tmp1 = _mm_packs_epi32(cmp1, cmp2);
	__m128i tmp2 = _mm_packs_epi32(cmp3, cmp4);
	uint32_t mask1 = _mm_movemask_epi8(tmp1);
	uint32_t mask2 = _mm_movemask_epi8(tmp2);

	uint32_t mask = (mask2 << 16) | mask1;

	int right_k = right_k_to;

	if (mask != 0)
	{
		right_k = (1 + __builtin_ctz(mask)) / 2;

		if (C[k][right_k] > nr)
			--right_k;
	}
	return right_k;
}
\end{lstlisting}

\subsection{Results}

We benchmarked our code using two types of benchmarks. The first type of benchmarks used
a \texttt{Google Benchmark} library. This is one of the standard libraries used to
microbenchmark code. Code snippet that is being benchmarked is run several times until
the stable results are obtained. This makes the obtained results reliable even if the
measured time is very small. The second type of benchmarks
were the ones that are included in \sdsl. These are built on top of a data from the
\texttt{Pizza\&Chili} datasets \citep{ferragina2005pizza} and include many types of data such
as DNA sequences, \texttt{C} and \texttt{Java} source codes from \texttt{Linux} and \texttt{GCC}
projects as well as English texts from the Gutenberg project. More information about this
data such as some statistics about the compressibility and much more can be found in
\cite{ferragina2009compressed}. We shall only provide information neccessary to explain the
measured phenomenons later when exploring the results of our benchmarks. All the benchmark
results were obtained on a machine with 8-core AMD~Ryzen~7~2700X with 16~MB of cache, running
on 3.7~GHz with 16~GB of RAM. The running operating system was Ubuntu~20.04.3~LTS.
We used both \texttt{GCC} version 9.4.0 and \texttt{Clang} version 10.0.0 compilers,
most of the time with the optimizations level set to O3. All the results we obtained are
easily reproducible just by following the instructions in AppendixTODO. However, to obtain
the best results possible, our implementation asks for a processor with a support for \texttt{SSE2}
instruction set.

\paragraph{Early RRR microbenchmarks}

These benchmarks were used in the early stage of the development to measure a potential gain from
our new proposed method of encoding and decoding. We mainly focused on measuring different block sizes
in combination with different methods used for dividing block to subproblems and with different
approaches to obtain particular block size. The mainly tested block sizes were 15, 30, 31, 62, 63
and 127. Sizes 15, 31, 63 and 127 are most useful in practice however they can be obtained using
different combinations of block size 30 and 62, i.e. 31 can be combined as 1 bit and 30 bit solution,
63 can be divided to 1 and 62 as well as to 3 and two sub-problems of length 30.
We used 3 different techniques to divide a problem into sub-problems, these were the linear scan,
binary search and linear scan enhanced by \texttt{SIMD} instructions.

\paragraph{Bit vector}

After bencharking and finding the potential for speeding up the bit vector implementation using
our new method, we decided to implement and test it in environment that is closer to the real-life
usage of bit vectors. We picked a benchmarking part of \texttt{SDSL} library that tests bit vector
on a randomly generated sequences with fixed number of ones and on raw representation of a wavelet
tree built on top of data from the DNA sequence, XML web data taken from \texttt{DBLP}, computer
science bibliography. Among the explored dimensions is block size, size of the underlying data.
Through all of our experiments, we did not made any changes to the preset number of blocks per
superblock that is set in \texttt{SDSL} to 32. \texttt{SDSL} uses the random pattern for $\access$,
$\rank$ and $\select$ queries. We were not able to find significant practical usability for other
access patterns. As a baseline, we choose the 15 bit specialized implementation already implemented
in \texttt{SDSL} that uses the table decoding method.

\paragraph{Bit vector in FM-index}

We further benchmarked our bit vector implementation as a part of Huffman shaped wavelet tree
used inside of the FM-index. Implementation of FM-index in \texttt{SDSL}, as most of other
implementations of FM-index, provides 3 basic methods:

\begin{itemize}
	\item $\mathit{count}(P)$ counts the number of occurrences of $P$ in text $T$
	\item $\mathit{locate}(P)$ returns all positions of pattern $P$ in text $T$
	\item $\mathit{extract}(i, j)$ returns the subsequence $T[i..j]$
\end{itemize}

The reason that the $\mathit{extract}$ method is useful and non-trivial is that FM-index
does not store the original sequence $T$ -- at least not in an easily readable form.
As in previous case, these methods are benchmarked on samples like English texts, \texttt{XML}
data from \texttt{DBLP} as well as on source codes, DNA and protein sequences and many more.
We present here just a selected subset of the results obtained. We either choose samples
where our implementation beat the original implementation by a wide margin or others where
our implementation was not running that good. All other graphs can be found in AppendixTODO.

\section{Hybrid encoding}

\subsection{Implementation}

Implementing hybrid encoding required changes to more than encoding and decoding
subroutines. The first neccessary change is that we added an hybrid cutoff parameter
to the \texttt{rrr\_vector} class. The second change relates to the way how we work
with superblocks. When computing rank of a bit in particular block, we can still binary
search for the superblock where the $i$-th one is located.  To linearly search for a result
inside of the superblock, we need just information from the array $C$. This is because we know
the number of ones in the block using this array and can continu through the superblock.
When we overrun the block, where the result is definitely located, we can go back.
Now using the precomputed offset of this superblock into the array $O$ and information we kept
during the linear scan, we can easily find the beginning of the block that is interesting
for us. With cutoff in place however, we are not able to linearly scan through the superblock
as we need to know the number of ones in the block. However, if number of ones in the block
is more than cutoff, we need to access array $O$ and perform counting of ones in it.

\subsection{Results}
