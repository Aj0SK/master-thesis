\chapter{Implementation and benchmarking}
\label{kap:kap4}

In this chapter, we focus on the implementation of the ideas from previous chapter
and the results we obtained using them. First, we describe the important parts and
decisions in our implementation. Then, we describe how we benchmarked our implementation
and the results we obtained.

\section{Implementation}

We have decided to make our solution a part of the SDSL library~\citep{gog2014theory}. This
is one of the most matured and versatile libraries implementing succinct data structures.
SDSL is a heavily tested library offering various implementations of succinct
structures such as wavelet tree, FM-index, bit vectors, suffix array and many more.
It allows to easily use different building blocks to implement more complex structures,
i.e. using different bit vector implementations in wavelet tree. On top of this, thorough
tests and benchmarks were devised, mainly by \cite{gog2014optimized}.
The RRR implementations consists of templated class \texttt{rrr\_vector} that uses the on the fly-decoding
and enables us to use any block size from 3 up to 256. SDSL also provides specialization
of this class for block size 15 that uses the table encoding method. We thus decided to use
this specialization as an underlying solution for the sub-blocks encoding and decoding. we further
provided specialized implementations for block sizes 31, 63 and 127 that are mostly used in the
practical scenarios. We based our specializations on the the general implementation of
\texttt{rrr\_vector} and tried to keep the number of changes as small as possible. We have been able
to only alter the two functions that the SDSL for encoding and decoding namely \texttt{bin\_to\_nr}
and \texttt{nr\_to\_bin}. As we mentioned, the encoding is less performance critical for most of the
applications as encoding is done only once at the beginning. This is why we focused more on the
decoding implementation.

\paragraph{Division to subproblems}

When implementing our method, we identified the proces of dividing our problem to sub-problems
as the most critical. There are varios reasons why. The first one is that for smaller blocks 
solving the sub-problems is done using the table approach that is quite fast and can be hardly
made faster as it consists only of one lookup to table. The second one is that this part is
inherently blocking us from running the methods on the sub-problems. These sub-problems may be
solved in parallel (either by more workers or by instruction-level parallelism), this part is
though harder to parallelize. Dividing the problem into sub-problems consists of
finding the pair of classes $(c_1, c_2)$ for our block $B$. After this, we need to find the number
of ways how the first and second sub-block may look.
These numbers can be precomputed as for every class $c$, there is at most $b+1$ ways how to
write it in a form $c_1+c_2$ for valid $c_1$ and $c_2$. After obtaining the number of combinations
for the first and second sub-block, it is then trivial to compute the parameters for sub-routines 
that decode the sub-blocks and combine their result to obtain decoded block. We devised 3 different
methods to find the pair $(c_1, c_2)$ for our block $B$. We may precompute how many block there are
with class pair equal to $(0, c), (1, c-1),\ldots , (c, 0)$ and name these numbers $C_{0}\ldots C_{c}$.
Example is on Fig.~\ref{table:class_pairs}.
\begin{figure}
	\centerline{
        \begin{tabular}{c c c c}
            $C_k$	&	$(c_1, c_2)$  &   Block count & Offsets of blocks\\
        \hline
			$C_0$	&	$(0, 2)$	&   \tt 3 	&	0-2\\
			$C_1$	&	$(1, 1)$	&	\tt 9 	&	3-11\\
			$C_2$	&	$(2, 0)$	&	\tt 3	&	12-14\\
        \end{tabular}
	}
	\caption[TODO]{
        Table shows all the class pairs with the number of blocks for $b=6, c=2$.
    }
	\label{table:class_pairs}
\end{figure}
We would like to map the offset of the block to the number $C_i$.
One possible way is to precompute the prefix sums, such that $P_i = \sum_{i=0}^{i} C_i$.
As these prefix sums form an increasing sequence, we can binary search for class
that contains our offset. Although binary search has better time complexity, as the number
of buckets where our offset may land is usually small, linear search outperforms the binary
most of the time. In practice, we found that sequential search using the \texttt{SIMD}
instructions leads to the best results for a smaller block sizes.

\section{Benchmarking}

We benchmarked our code using two types of benchmarks. The first type of benchmarks used
a \texttt{Google Benchmark} library. This is one of the standard libraries used for
microbenchmarking of code. It can reliably benchmark code as it does a lot of work
under the hood. Code snippet that is being benchmarked is run several times until the
library can reliably state some information about its runtime. The second type of benchmarks
were the ones that are included in \texttt{SDSL}. These are built on top of a data from the
\texttt{Pizza\&Chili} datasets \citep{ferragina2005pizza} and include many various datasets such
as DNA sequences, \texttt{C} and \texttt{Java} source codes from \texttt{Linux} and \texttt{GCC}
projects as well as English texts from the Gutenberg project. More information about this
data such as some statistics about the compressibility and much more can be found in
\cite{ferragina2009compressed}. We shall only provide information neccessary to explain the
measured phenomenons later when exploring the results of our benchmarks. All the benchmark
results were obtained on a machine with 8-core AMD~Ryzen~7~2700X with 16~MB of cache, running
on 3.7~GHz with 16~GB of RAM. The running operating system was Ubuntu~20.04.3~LTS.
We used both \texttt{GCC} version 9.4.0 and \texttt{Clang} version 10.0.0 compilers,
most of the time with the optimizations level set to O3. All the results we obtained are
easily reproducible just by following the instructions in AppendixTODO. However, to obtain
the best results possible, our implementation asks for a processor with a support for \texttt{SSE2}
instruction set.

\paragraph{Early RRR microbenchmarks}

These benchmarks were used in the early stage of the development to measure a potential gain from
our new proposed method of encoding and decoding. We mainly focused on measuring different block sizes
in combination with different methods used for dividing block to subproblems and with different
approaches to obtain particular block size. The mainly tested block sizes were 15, 30, 31, 62, 63
and 127. Sizes 15, 31, 63 and 127 are most useful in practice however they can be obtained using
different combinations of block size 30 and 62, i.e. 31 can be combined as 1 bit and 30 bit solution,
63 can be divided to 1 and 62 as well as to 3 and two sub-problems of length 30.
We used 3 different techniques to divide a problem into sub-problems, these were the linear scan,
binary search and linear scan enhanced by \texttt{SIMD} instructions.

\paragraph{Bit vector - SDSL}

After bencharking and finding the potential for speeding up the bit vector implementation using
our new method, we decided to implement it and test in some environment closer to the real-life
usage of bit vectors. We picked a benchmarking part of \texttt{SDSL} library that tests bit vector
on a randomly generated sequences with fixed number of ones and on raw representation of a wavelet
tree built on top of a data from the DNA sequence, XML web data taken from \texttt{DBLP}, computer
science bibliography. Among the explored dimensions is block size, size of the underlying data.
Through all of our experiments, we did not made any changes to the preset number of blocks per
superblock that is set in \texttt{SDSL} to 32. \texttt{SDSL} uses the random pattern for $\access$,
$\rank$ and $\select$ queries. We were not able to find significant practical usability for other
access patterns. As a baseline, we choose the 15 bit specialized implementation already implemented
in \texttt{SDSL} that uses the table decoding method.

\paragraph{Bit vector in FM-index}

We further benchmarked our bit vector implementation as a part of Huffman shaped wavelet tree
used inside of the FM-index. Implementation of FM-index in \texttt{SDSL}, as most of other
implementations of FM-index, provides 3 basic methods:

\begin{itemize}
	\item $\mathit{count}(P)$ counts the number of occurrences of $P$ in text $T$
	\item $\mathit{locate}(P)$ returns all positions of pattern $P$ in text $T$
	\item $\mathit{extract}(i, j)$ returns the subsequence $T[i..j]$
\end{itemize}

The reason that the $\mathit{extract}$ method is useful and non-trivial is that FM-index
does not store the original sequence $T$ -- at least not in an easily readable form.
As in previous case, these methods are benchmarked on samples like English texts, \texttt{XML}
data from \texttt{DBLP} as well as on source codes, DNA and protein sequences and many more.
We present here just a selected subset of the results obtained. We either choose samples
where our implementation beat the original implementation by a wide margin or others where
our implementation was not running that good. All other graphs can be found in AppendixTODO.