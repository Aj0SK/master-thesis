\chapter{History and recent advancements}
\label{kap:kap1}

The size of information and data grows exponentially in recent years. In order to store this data, we use different data structures. These data structures not only represent the data but also enable us to make certain operations over them. There is a significant effort in not only speeding up these structures but also making them smaller in size so that they can be managed easier in memory and not on secondary data storage thas is often orders of magnitude time slower.

\section{Space-efficient data structures}

When we measure how space-efficient the data structure is, we often measure this in additional information that we store alongside the original data. Most of the time, with the complexity of operations that our data structure supports also the size of metadata stored increase. The original data could be however compressed to further increase the space-efficiency. To remove ambiguity, we measure the size of the original data as the number of the information-theoretical optimal number of bits used to store the original data. We will now introduce the basic definition that distinguishes between space-efficiency of data structure:

Let's say that X is the information-theoretical optimal number of bits needed to store the data. A data structure representing this data is called:
\begin{itemize}
\item implicit if it takes $X + O(1)$ bits of space
\item succinct if it takes $X + O(X)$ bits of space
\item compact if it takes $O(X)$ bits of space
\end{itemize}

As an example, the classical representation of string in language as C uses the null-terminated method. In this form, the original string is stored in memory with the null termination character at the end. It is easy to see that this representation is implicit. To represent the string we could also store the string and only remember it's length, not using the null termination character. This would, however, lead to a succinct data structure as we need $log(X)$ bits to store the length of the string.

It should be clear that the additional requirement on the data structure to be space-efficient many times leads to worse time complexity. Even if the theoretical time complexity remains the same for compressed data structures, the real-world implementation can suffer in performance as the compressed methods are many times not ideal in the classical computer model.

\section{History and recent progress in the field}

We will now look briefly into the history and then on recent results in this topic. The field dates back to the Jacobsen work on static succinct data structures.

There are of course many possible data structures and operations that they can support in order to be reasonable to use. We will now look at the most common operations that are supported in the succinct data structures.

\begin{theorem}
Let's say that we have a finite sequence of $N$ elements.
We define two operations rank and select: \\
$rank_q(x) = | \{ k \in [ 1, 2, \ldots, x]:N[k] = q  \} |$ \\
$select_q(x) = min \{ k \in [ 1, 2, \ldots, n):rank_q(k)=x  \} $
\end{theorem}

In other words, $rank_q(x)$ denotes the number of elements in $N$ that are equal to $q$ and their position in $N$ is less than $x$. $select_q(x)$ returns the position of $x$-th occurence of $q$.
These two types of operations are very common in succinct data structures. Lets say we want to store the sequence of elements that are not of fixed size.
With the sequence of data, represented by the individual bits of elements. We can also store metadata in the form of bitvector, that has one's on the indexes
where the new elements begins as we see in Fig. \ref{obr:obr_rank_select}.

\begin{figure}
\centerline{\includegraphics[width=0.4\textwidth]{images/obr_rank_select}}
\caption[Rank select usage in representation of sequence of elements with different size]{Rank select used in metadata, denoting start of the element.}
\label{obr:obr_rank_select}
\end{figure}

\section{Dynamic succinct data structures}

\subsection{K-th order entropy}

\begin{theorem}
Let's have a string $S$ with size $n$ and $n_i$ denoting number of occurences of character
on position $i$. Empirical entropy of string $S$ is denoted $H_0(S)$ and defined as:
\begin{center}
$H_0(S) = \frac{1}{n} \sum_{i=1}^{k} n_i\cdot log(\frac{n}{n_i})$
\end{center}
\end{theorem}

Most of these algorithms use as a measure of information stored a $k$-order empirical entropy.

\begin{theorem}
Let's have a string $S$ and construct $S_w$ as a string obtained by concatenating the characters immidiately following occurences of $w$ in $S$.
$k$-order empirical entropy ($k \geq 1$) of $S$ denoted $H_k(S)$ is defined as:
\begin{center}
$H_k(S) = \frac{1}{n} \sum_{|w|=k} |S_w| H_0(S_w)$
\end{center}
\end{theorem}

This is a measure of information that is emitted with respect to the previous $k$-characters.
