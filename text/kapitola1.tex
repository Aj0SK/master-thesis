\chapter{Introduction to succinct data structures}
\label{kap:kap1}

\section{Motivation}
Data structures are one of the key concepts in computer science. They are essential
as we use them to store the ever-growing amount of data and information. In many
applications, the amount of data is so significant that we optimise how much storage
these structures use. The field of succinct data structures focuses on representing
data using as little space as possible while trying not to hurt the runtime of methods
that these data structures support. In this field, many interesting data structures
have been already devised. While many of these give some solid theoretical bounds on
the used space, others look into real-world space usage and performance. The problem
is that these structures are often more advanced than their space non-efficient alternatives
and contain concepts and patterns that are slow on modern computers.

The primary use case of these structures comes in two flavours. The first case is
that we have so much data and are trying to implement some useful methods working
with them. The problem we face with regular data structures supporting these
operations is that they may not even fit into our entire physical memory. The
second use case may be that even if we have data that could fit into our computer,
we instead use a succinct representation that may enable us to store the data in
a faster type of memory (e.g. fast RAM instead of the slower disk) thus even if we
use more advanced concepts, the overall runtime may decrease.

The work in this field turns more and more actively now at the dynamic succinct
data structures. These are structures where the underlying data may change after
constructing the structure. However, in this work, we will only consider static
succinct data structures. These are the ones where we get the data, then construct
the succinct data structure and then only support methods that do not alter the
underlying data in any way.  

\section{FM-index}

One particularly useful succinct data structure is FM-index. Let us consider the
problem of representing a text that we want to store effectively. On top of that,
we want to efficiently count or locate occurrences of pattern $P$ in this text.
FM index is able to do this in time complexity $\mathcal{O}(|P|)$ after preprocessing the
original text. On top of this, the resulting data structure is for many applications
smaller than the original text. This makes it particularly useful for searching in
long texts of DNA where FM-index many times takes just 30-40\% of the space needed
for the representation of original text. We shall now look at how the FM-index
is obtained for the text $T$. For simplicity, we assume that $T$ contains a special
character \$ only at the end that is contained in $\Sigma$ and lexicographically smallest.

\subsection{Burrows-Wheeler transformation}

Burrows-Wheeler transformation (BWT) lies at the heart of the FM index, so we will
look into how it is constructed. We shall consider the sequence $S$ of symbols over
some alphabet $\Sigma$. Now we take all rotations $S_1, S_2, \ldots S_n$ of the
sequence $S$ in a lexicographically sorted order and name the concatenation of
first(last) symbols $F$ ($L$). Note that $F$ can be stored using one integer per
symbol of the alphabet as $F$ contains only runs of sequence symbols. These runs
are sorted according to the lexicographical ordering of alphabet symbols. We will
use helper sequence $Count$ where $Count[c]$ is number of occurrences of $c\in\Sigma$ in $S$.

\subsection{LF mapping}

The main idea behind the FM index is how to obtain what we call LF mapping. This
is a mapping that maps $i$-th row in $L$ to the corresponding row in $F$ so that
$L[i] = F[j]$. Although $Count$ uniquely represents $F$ we will find also useful
array $StartofRun$ where $StartofRun[c]$ is a total number of occurrences of symbols
smaller than $c$. We note that $StartofRun[c]+1$ is index where run of symbol $c$
will begin in $F$. We need now a fast implementation of LF mapping. At first, we
assume that every symbol is in sequence at most once. It is now easier to implement
LF mapping. It is good to note that now all the runs in $F$ vector will be of length 1.
How do we find $LF(i)$? We will simply look at $i$-th element in $L$. The run of
symbol $L[i]$ begins on the index $StartOfRun[L[i]] + 1$ so the formula can be written as:

$$LF(i) = StartOfRun[L[i]] + 1$$.

If we drop our requirement that every symbol is at most once in the sequence $S$
we are seemingly in a more complicated situation. The problem may seem that even
though we still easily identify the beginning of the run of $L[i]$, we do not
know which occurrence of symbol $L[i]$ corresponds to this occurrence of $L[i]$.
Nice property is that $i$-th occurrence of symbol $c$ in $L$ corresponds to the
$i$-th occurrence of $c$ in $F$. So we can just change the final formula to:

				$$LF(i) = StartOfRun[L[i]] + rank_{L[i]}(i)$$

where $rank_c(i)$ represents the number of occurences of $c$ up to $i$-th index.

\subsection{Supported methods}

As we already stated, FM-index is used for searching in the preprocessed text.
It does this using three methods:

\begin{itemize}
\item $locate(P)$ returns all positions of pattern $P$ in text $T$
\item $count(P)$ counts the number of occurences of $P$ in text $T$
\item $extract(i, j)$ returns the subsequence $T[i..j]$
\end{itemize}

The reason that the $extract$ method is useful and non-trivial is that FM-index
does not store the original sequence $T$ - at least not in an easily readable form.

%TODO: Implementacia operacii

\section{Storing elements of non-uniform size}

When trying to optimise the space used, it may be beneficial to use variable-length
code such as Huffman code. The problem is that even storing elements like this in
the array is problematic. With the variable-length elements, we do not have a trivial
and fast way to tell where $i$-th element is located. Even though there are many
solutions, one of them that is used quite often is storing these elements one after
another in memory. Then alongside this raw sequence of bits, we have another helper
bit-vector of the same size. This bit vector has ones on places where the representation
of the element starts and ones on the other places.


In the first row, we can see the elements stacked one after another. These elements
are of variable size as they need 4, 4, 2, 1 and 2 bits respectively to be stored.
Under the elements from the main vector, there is another helper bit-vector. Every
symbol of one in the helper vector corresponds to one element in the main vector
and is positioned in a place where the element's representation begins in the main vector.

To find the beginning of $i$-th element, we now transformed this task to find the
$i$-th one in the helper array. We shall call this method $select_c(i)$ and later
see how it can be implemented efficiently.

\section{Wavelet tree}
We shall assume for a moment that we have a bit-vector implementation supporting
access, rank and select methods. We will now see how this may be used to create
a more general version of this implementation representing sequence over arbitrary
alphabet still supporting access, rank and select queries in reasonable time complexity.

Wavelet tree uses a divide-and-conquer approach. It takes the sequence $S$ of
length $n$ over some alphabet $\Sigma$ and recursively splits the alphabet into
two subsets creating a hierarchical partitioning of an alphabet. In the top node
of the tree, it splits the alphabet $\Sigma$ into two subsets of the roughly same
size $\Sigma_1, \Sigma_2$. It then stores a bit vector $B$ of size $n$ in this node
where $B[i] = 0 if S[i] \in \Sigma_1 else 1$. It then recursively applies the same
idea on a subsequence of $S$ that is created from symbols of $Sigma_1$ ($Sigma_2$)
to create the left (right) child node.


We can see how the recursive partitioning of the alphabet works. In every node,
we see also the subsequence represented in the subtree of the node even if this
sequence is not stored. 

Rank and select methods on the original sequence can be implemented using the tree
traversal and rank/select methods on the individual bit-vectors. Another modification
studied in [WaveletTreeHuffman] is to shape the binary tree in a way that a Huffman
tree of the sequence symbols is shaped. Huffman tree is a tree constructed in the
process of creating Huffman encoding of the characters contained in the sequence.

\section{Space efficiency}

There are many ways to measure the space efficiency of a data structure. From the
practical point of view, we may be interested in the memory usage by the data
structure on actual data that the structure could encounter in our use case. However,
many succinct data structures are trying to achieve some upper bounds and express
the used space in terms of the information contained in the data. There are many
models how to measure the information contained in sequences of characters. One
of the most used models in the field of succinct data structures, yet still quite
simple, is entropy or zeroth-order entropy:

$$H(S)=\sum_{c\in\Sigma} \frac{n_c}{n} \lg \frac{n}{n_c}$$.

Many times, the stored sequence is compressible using the sole fact that some symbols
have a bigger frequency than others.

\cite{raman2007succinct}
