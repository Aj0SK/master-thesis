\chapter{Introduction to succinct data structures}
\label{kap:kap1}

\section{Motivation}

In many applications, the amount of data is so significant that we optimise how much
space is used by our data structures. The field of \textit{succinct data structures}
focuses on representing data using as little space as possible while trying not to hurt
the runtime of methods that these data structures support. In this field, data structures
for many varied problems have been already devised. While many of these give some solid
theoretical bounds on the used space, others look into real-world space usage and
performance.

The most common use case for the succinct data structures is a situation when we
have a lot of data and are trying to implement some useful methods working
with them. The problem we face with regular data structures supporting these
operations is that their representation may not fit into our entire physical
memory. As succinct data structures need to do the work of regular data structures but
with additional constraints on memory usage, the common disadvantage is that
they are often slower and more complicated than their space non-efficient
alternatives. This is mainly because they do more work and at the same time contain
concepts and computation patterns that are slow on modern computers. However, there
are cases in which usage of space-efficient data structures may speed up the overall
running time as they may enable us to store the data in a faster type of memory
(e.g. fast RAM instead of the slower disk). Thus, even if they use slower, more
advanced concepts, the overall runtime may decrease.

The work in this field turns focus more and more towards the dynamic succinct
data structures. These are structures where the underlying data may change after
constructing the structure. However, in this work, we only consider static
succinct data structures. These are the ones where we get the data, then construct
the succinct data structure and then only support methods that do not alter the
underlying data in any way.

In this work, we are mainly concerned about the \textit{bit-vector}. One of the simplest
succinct data structures that represents the sequence of zeroes and ones. The reason
behind this is that bit-vector is one of the main building blocks of many succinct data
structures and can be useful in a lot of use cases. We shall now look onto some interesting
and useful applications of the bit-vector. 

\section{Storing elements of non-uniform size}

When trying to optimise the space used, it is often beneficial to use variable-length
code for individual symbols. The example of this is a Huffman code that constructs the
code for symbols based on the frequencies with which the symbols occur. Although working
with these symbols saves space, even storing these elements and allowing accessing $i$-th
is not trivial. Even though we can store these elements one after another in memory, with
the variable-length elements, we do not have an easy and fast way to tell at which offset
from the beginning of the array $i$-th element is located. Even though there are many
solutions, one of them that is used quite often is as follows. Alongside this raw sequence
of bits that store the elements one after another, we have another helper bit-vector of
the same size. This bit vector has ones on places where the representation of some element
starts and zeroes on the other places.

Identifying the beginning of $i$-th element now boils down to efficiently finding the
$i$-th one in the helper array. We shall call this method select and mark the answer
to this query $select_1(i)$. In chapter \ref{kap:kap2} we see how this method can be
implemented efficiently.

\begin{figure}
	\centerline{
	\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|}
	\cline{2-11}
	\textbf{Raw binary representation} & 1          & 1 & 0 & 1 & 1          & 1 & 1          & 0 & 1 & 0          \\ \cline{2-11} 
	\textbf{Helper bit array}          & \textbf{1} & 0 & 0 & 0 & \textbf{1} & 0 & \textbf{1} & 0 & 0 & \textbf{1} \\ \cline{2-11} 
	\end{tabular}
	}
	\caption[TODO]{Raw binary representation of elements 1101, 11, 101 and 0 stored one after another.
	Note the helper bit array that is of the same size with ones on the positions where new element begins.}
	\label{obr:VariableSizedElements}
\end{figure}

\section{FM-index}
% sources to help understand https://www.dcc.uchile.cl/TR/2005/TR_DCC-2005-004.pdf

Let us now consider another interesting problem. As we will see, solution to this problem
also uses bit-vector and other building blocks commonly used in succinct data strucutes.
This time, we have a text $T$ and are interested in indexing it. This means if we get some
pattern $P$, we would like to answer how many times is $P$ contained in $T$ and also where
in $T$ this occurrences are located. This is particularly interesting problem in bioinformatics,
where we have very long sequence of DNA and are interested in searching some subsequences
in it. One of the solutions that is reasonable and may be used is constructing
\textit{suffix array}. The suffix array of $T$, $S$ is array of integers storing
the starting positions of suffixes of $T$. If $P$ is contained in $T$ it will be at the
beginning of some suffixes that make up a continuous subsequence of $S$. Succinct data
structure that is similar to suffix array was proposed by \cite{ferragina2000opportunistic}
and is called \textit{FM-index}. FM-index can find the pattern in time complexity
$\mathcal{O}(|P|)$ after preprocessing the original text. On top of this, the resulting
space used by FM-index is for many applications smaller than the space used for the
original text $T$. This makes FM-index particularly useful for searching in very long sequences
of text where FM-index many times takes just 30-40\% of the space needed for the representation
of original text $T$ as was demonstrated by \cite{ferragina2001experimental}. We shall
now look at how the FM-index is obtained for the text $T$.

\subsection{Burrows-Wheeler transformation}

For simplicity, we assume that text $T$ contains a special character \$ located at its end
that is contained in $\Sigma$ and lexicographically smallest.

\textit{Burrows-Wheeler transformation} (BWT) lies at the heart of the FM-index.
BWT of a text $T$ gives us a sequence $T_{BWT}$. Furthermore, this
operation is reversible in a sense that only using $T_{BWT}$ we are able to reconstruct the
original text $T$. This transformation is used as a preprocessing step of some compression
algorithms such as bzip2 introduced by \cite{seward1996bzip2} as BWT is many times more
compressible. We show how it is constructed and why it has these properties used. We consider
the sequence $S$ of symbols over
some alphabet $\Sigma$. Now we take all the cyclical rotations $S_1, S_2, \ldots S_n$ of the
sequence $S$ and imagine putting them into the rows of table $M$ sorted by their lexicographical
order as seen in Fig. \ref{obr:BWT}. We name the sequence created by the concatenation of symbols
in first and last column $F$ and $L$ respectively. Last column $L$ is what we also call
the BWT transformation of $S$. We will show how to get $S$ from $S_{BWT}$ in the next section. The sequence $F$ can
be obtained from $L$ just by sorting all the characters in $S_{BWT}$. Note that $F$ has some
nice properties. It consists of of runs of symbols that are sorted according to the lexicographical
ordering of alphabet symbols. We will use the helper sequence $Count$ where $Count[c]$ is
number of occurrences of symbols smaller than $c$.

\begin{figure}
	\centerline{
	\begin{tabular}{l|c|ccccc|c|}
	\cline{2-8}
	  & \multicolumn{1}{l|}{\textbf{F}}    & \multicolumn{5}{l|}{}                                                                                                                                                                                                         & \multicolumn{1}{l|}{\textbf{L}}    \\ \cline{2-8} 
	1 & {\color[HTML]{333333} \textbf{\$}} & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} b}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} a}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} n}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} a}}  & {\color[HTML]{C0C0C0} n}  & {\color[HTML]{000000} \textbf{a}}  \\ \cline{2-8} 
	2 & {\color[HTML]{333333} \textbf{a}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} \$}} & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} b}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} a}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} n}}  & {\color[HTML]{C0C0C0} a}  & {\color[HTML]{000000} \textbf{n}}  \\ \cline{2-8} 
	3 & {\color[HTML]{333333} \textbf{a}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} n}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} a}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} \$}} & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} b}}  & {\color[HTML]{C0C0C0} a}  & {\color[HTML]{000000} \textbf{n}}  \\ \cline{2-8} 
	4 & {\color[HTML]{333333} \textbf{a}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} n}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} a}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} n}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} a}}  & {\color[HTML]{C0C0C0} \$} & {\color[HTML]{000000} \textbf{b}}  \\ \cline{2-8} 
	5 & {\color[HTML]{333333} \textbf{b}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} a}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} n}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} a}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} n}}  & {\color[HTML]{C0C0C0} a}  & {\color[HTML]{000000} \textbf{\$}} \\ \cline{2-8} 
	6 & {\color[HTML]{333333} \textbf{n}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} a}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} \$}} & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} b}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} a}}  & {\color[HTML]{C0C0C0} n}  & {\color[HTML]{000000} \textbf{a}}  \\ \cline{2-8} 
	7 & {\color[HTML]{333333} \textbf{n}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} a}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} n}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} a}}  & \multicolumn{1}{c|}{{\color[HTML]{C0C0C0} \$}} & {\color[HTML]{C0C0C0} b}  & {\color[HTML]{000000} \textbf{a}}  \\ \cline{2-8} 
	\end{tabular}
	}
	\caption[TODO]{All the cyclic rotations of sequence $S = banana\$$. BWT is a $L$ - last column. Note that we are not storing this whole table.}
	\label{obr:BWT}
\end{figure}

The reason that BWT is better compressible is that it frequently contains runs of
the same symbol. This can be explained on an example. Let us create BWT of a text containing
a lot of mentions of word computer. All the rotations starting with omputer will be grouped
in $M$ into one run with most of them ending in c as this is the most common symbol
preceding word omputer.

\subsection{LF-mapping}

Useful information, that will be used in the future is \textit{LF-mapping}.
This is a mapping that for a symbol on $i$-th row in $L$, gives us a corresponding row in $F$
where this exact symbol is located. This is quite easy to obtain if the underlying sequence $S$
does not contain duplicate symbols. Now all the runs in $F$ vector will be of length 1.
To find $LF(i)$, we will simply look what is the $i$-th symbol in $L$. The run of
symbol $L[i]$ begins on the index $Count[L[i]] + 1$ so the formula can be written as:

$$LF(i) = Count[L[i]] + 1$$.

If we drop our requirement that every symbol is contained in the sequence $S$ at most once
we are seemingly in a more complicated situation. The problem may seem that even
though we still easily identify the beginning of the run of $L[i]$, we do not
know which occurrence of symbol $L[i]$ corresponds to the occurrence of $L[i]$ in $i$-th row.
Helpful property is that $i$-th occurrence of symbol $c$ in $L$ corresponds to the
$i$-th occurrence of $c$ in $F$. So we can just change the final formula to

				$$LF(i) = Count[L[i]] + rank_{L[i]}(i)$$

where $rank_c(i)$ represents the number of occurrences of $c$ up to $i$-th index inclusive.

\subsection{Reverse Burrows-Wheeler transformation}

We already stated that the BWT is reversible. So from the $T_{BWT}$ we are able to reconstruct
$T$. We do it by first obtaining $F$ from $T_{BWT}$. Now, we will iteratively reconstruct $S$
from the end to the beginning. First, we look to the first position in $F$ where last symbol
of $S$ - \$ - is located. We see what symbol is preceding \$ in $S$. This is a symbol
$L[1]$. Using the LF-mapping we find where $L[1]$ is located in $F$ and then repeat this process
until $L[i]$ is equal to \$.

\subsection{Supported methods}

As we already stated, FM-index is used for searching in the preprocessed text.
It does this using three methods:

\begin{itemize}
	\item $count(P)$ counts the number of occurrences of $P$ in text $T$
	\item $locate(P)$ returns all positions of pattern $P$ in text $T$
	\item $extract(i, j)$ returns the subsequence $T[i..j]$
\end{itemize}

The reason that the $extract$ method is useful and non-trivial is that FM-index
does not store the original sequence $T$ - at least not in an easily readable form.

The counting of occurrences of $P$ in $S$ is based on the observation that in $M$,
all the suffix are contained and come in sorted order. Occurrences of $P$ in $M$
are contained in a consecutive subsequence of $M$'s rows. The identification of
rows where $P$ is located will proceed gradually by finding rows where $P[n-1..],
P[n-2..]$ etc. is located. This will in general be some continuous subsequence of
rows of $M$ so we will store its beginning and end.

\begin{itemize}
	\item We find where $P[n-1]$ is located in $M$. This means setting $b_{n-1}$ to
	$Count[P[n-1]]+1$ and $e_{n-1}$ to $Count[P[n-1]+1]$.
	\item Now, to found where $P[n-2..]$ is located in $M$, we would like to locate
	occurrences of $P[n-1]$ that are preceded by $P[n-2]$. We would like to find all
	occurrences of $P[n-2]$ in $L[b_{n-1}..e_{n-1}]$. Hovewer, this is not easy to do
	as they can be at arbitrary places in this subsequence, even not in a continuous
	sequence. Instead we look at the occurrences of $P[n-2]$ in $F$. Here they are
	sorted and it is easy to tell, which of these are followed by $P[n-1]$ as $i$-th
	occurence of character $P[n-1]$ in $L$ corresponds to $i$-th occurence of $P[n-1]$
	in $F$. Using this fact, we set
	$b_{n-2} = Count[P[n-2]] + rank_{P[n-2]}(b_{n-1})$ and
	$e_{n-2} = Count[P[n-2]] + rank_{P[n-2]}(e_{n-1})$.
	\item We continue until we compute $b_1$ and $e_1$ or until $b_i\neq e_i$.
\end{itemize}

\section{Space efficiency}

There are many ways to measure the space efficiency of a data structure. From the
practical point of view, we may be interested in the memory usage by the data
structure on actual data that the structure could encounter in our use case. However,
many succinct data structures are trying to achieve some upper bounds and express
the used space in terms of the information contained in the data. There are many
models how to measure the information contained in sequences of characters. One
of the most commonly used models in the field of succinct data structures, yet still
quite simple, is entropy or more specifically zeroth-order entropy:

$$H(S)=\sum_{c\in\Sigma} \frac{n_c}{n} \lg \frac{n}{n_c}$$.

In many scenarios, where succinct data structures are used, the stored sequence
is compressible using the sole fact that some symbols have a bigger frequency than others.

\section{Wavelet tree}
\label{section:WaweletTree}

We shall assume for a moment that we have a bit-vector implementation supporting
access, rank and select methods. We show how this may be used to create
a more general version of this implementation representing sequence over arbitrary
alphabet still supporting access, rank and select queries in reasonable time complexity.

\textit{Wavelet tree} uses a divide-and-conquer approach. It takes the sequence $S$ of
length $n$ over some alphabet $\Sigma$ and recursively splits the alphabet into
two subsets creating a hierarchical partitioning of an alphabet. In the root node
of the tree, it splits the alphabet $\Sigma$ into two subsets of the roughly same
size $\Sigma_1, \Sigma_2$. It then stores a bit vector $B$ of size $n$ in this node
where
\[
    B[i]= 
\begin{cases}
    0,& \text{if } S[i]\in \Sigma_1\\
    1,              & \text{otherwise}
\end{cases}
\]

Then it creates two strings $S_1$ and $S_2$ from $S$ by taking just symbols
from $\Sigma_1$ and $\Sigma_2$ respectively. The left and right children of the root node
are then built by recursively applying the same idea on the subsequences $S_1$ and $S_2$.

\begin{figure}
	\centerline{
		\includegraphics[width=0.9\textwidth, height=0.3\textheight]{images/wavelet_tree}
	}
	\caption[TODO]{Wavelet tree representation of text $S=aabadacdc$. We can see how the
	recursive partitioning of the alphabet works. In every node, we also show the
	subsequence represented (grey text) in the subtree of the node. The stored data can be
	recognized as they are in bold and black.
	}
	\label{obr:WaveletTreeExample}
	% based on https://simongog.github.io/assets/data/sdsl-slides/tutorial#23
	% source at https://docs.google.com/drawings/d/1cJyda3bdTluajr3iXu1x1iL5HF0JPZqAHw6jwct9KLI/edit
\end{figure}

Rank and select methods on the original sequence can be implemented using the tree
traversal and rank/select methods on the individual bit-vectors. Another modification
studied by \cite{makinen2005succinct} is to shape the binary tree in a way that a Huffman
tree of the sequence symbols is shaped. Huffman tree is a tree constructed in the
process of creating Huffman encoding of the characters contained in the sequence.