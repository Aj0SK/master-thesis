\chapter{History and recent advancements}
\label{kap:kap1}

The size of information and data grows exponentially in recent years. In order to store this data, we use different data structures. These data structures not only represent the data but also enable us to make certain operations over them. There is a significant effort in not only speeding up these structures but also making them smaller in size so that they can be managed easier in memory and not on secondary data storage thas is often many times slower.

\section{Space-efficient data structures}

When we measure how space-efficient the data structure is, we often measure this in additional information that we store alongside the original data. Most of the time, with the complexity of operations that our data structure supports also the size of metadata stored increase. The original data could be however compressed to further increase the space-efficiency. To remove ambiguity, we measure the size of the original data as the number of the information-theoretical optimal number of bits used to store the original data. We will now introduce the basic definition that distinguishes between space-efficiency of data structure:

Let's say that X is the information-theoretical optimal number of bits needed to store the data. A data structure representing this data is called:
\begin{itemize}
\item implicit if it takes $X + O(1)$ bits of space
\item succinct if it takes $X + O(X)$ bits of space
\item compact if it takes $O(X)$ bits of space
\end{itemize}

As an example, the classical representation of string in language as C uses the null-terminated method. In this form, the original string is stored in memory with the null termination character at the end. It is easy to see that this representation is implicit. To represent the string we could also store the string and only remember it's length, not using the null termination character. This would, however, lead to a succinct data structure as we need $log(X)$ bits to store the length of the string.

It should be clear that the additional requirement on the data structure to be space-efficient many times leads to worse time complexity. Even if the theoretical time complexity remains the same for compressed data structures, the real-world implementation can suffer in performance as the compressed methods are many times not ideal in the classical computer model.

\section{History and recent advances in the field}

We will now look briefly into the history and then on recent results in this topic. The field dates back to the Jacobsen work on static succinct data structures.

\section{Dynamic succinct data structures}

