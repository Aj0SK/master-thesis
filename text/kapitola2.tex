\chapter{Binary sequence representation}
\label{kap:kap2}

Basic building block of many succinct data structures is \textit{vector}. Data structure
used to represent the sequence of characters over some alphabet $\Sigma$ with the
methods rank and select. As we
have shown in the section \ref{section:WaweletTree}, Wavelet tree can be used to
build vector for general alphabet from the bit-vector specialization. This is, why
it is of utmost importance to dedicate time to efficient and fast bit-vector
implementation. There are more ways to represent binary sequence. One of the widely
used technique is called \textit{RRR}. Named by its inventors Rajeev Raman, Venkatesh Raman,
S. Srinivasa Rao that proposed this data structure\cite{raman2007succinct}. We will look
at the first working RRR proposal of bit-vector\cite{claude2008practical}. Then we
shall look at the subsequent improvements over it \cite{navarro2012fast}.

\section{Division to blocks and access}

The main idea of RRR is to split the bit sequence into blocks of some constant
length $b$. This length is a parameter of algorithm and we will look at the consequences
of different choices of $b$. Then instead of storing the bit sequence we store the
block encoded as a pair of numbers $(c, o)$ where $c$ labels the class of this
block and $o$ an offset into the sequence of all blocks in this particular class.
Most of the time, the $c$ is equal to the number of ones in the block. Offset on
the other hand, is the index of block in the sequence of all the blocks with $c$ number
of ones, lexicographically sorted. Note that class $c$ has size

                $${b\choose c}$$.

The offset is then be bounded by:

				$$0 \leq o < {b\choose c}$$.

The process of obtaining $c$ and $o$ from block is called \textit{encoding}.
The opposite process is called \textit{decoding}. Although we would like both
processes to be fast. We do the encoding just once, at the initial construction
of bit-vector. On the other hand, decoding is done any time we are interested in
block's particular bit so it is more important for us to optimise so that decoding
is as fast as possible.

For smaller block sizes, such as $b\leq 15$, it was in most cases reasonable to
generate two helper tables. One named $T_e$ used for the encoding, where we can
index using the block casted as a number and get this particular block's offset.
The class can be computed trivially by counting number of ones. The other table
named $T_d$ is two dimensional. In position $T_d[c][o]$ we store the block that
is associated with $(c, o)$. In other words, this is block containing $c$ ones
and at the same time being $o$-th in the sequence of all lexicographically sorted
blocks containing $c$ ones. Both these tables $T_e$ and $T_d$ can be computed
by generating all the possible blocks in lexicographical order before we even
obtain the sequence that we will want to represent. After this precomputation,
the encoding and decoding of a block takes constant time.

After slicing the sequence $S$ to blocks, we need to find a way how to store
pairs $(c, o)$. It is reasonable to store all the classes and offsets in separate
arrays $C$ and $O$. The array $C$ is usually implemented as an array of fixed length
elements with length equal to $\lceil \log_2(b)\rceil$. The array $O$ is implemented as an
array of variable length elements and the size needed to store $i$-th element $O[i]$ is
$\log_2({b\choose C[i]})$. To decode the $i$-th block, we at first need to get
$C[i]$ and $O[i]$ and then we can use a precomputed table $T_d$ that matches $(c_i, o_i)$
to the original block. Obtaining $C[i]$ is trivial as it is at a fixed offset in
memory. Getting value of $O[i]$ is harder as it is not at a known offset but generally
on an offset that can be expressed as

                $$\sum_{j=1}^{i-1} \log_2{b\choose C[i]}$$.

We need to basically one by one skip over elements that come before $O[i]$.
Note that to move in $O$ we need only successive information from $C$ so these
are quite cache friendly. However, to access the $i$-th block, we need in worst
case to look at all the elements of $C$ and this takes $\mathcal{O}(n/b)$ time. To support
the access, we need to store the arrays $C, O$ and helper table $T_d$. We shall
now argue about the size of these structures. The size of these structures are
as follows:

$C$ is array of $\lceil n/b \rceil$ elements of fixed size $\lceil \log(b+1) \rceil$.

For array $O$ we argue that its size is bounded by

$$\sum_{i=1}^{n/b} \bigg\lceil{\log b\choose c_i}\bigg\rceil =
\sum_{i=1}^{\lceil n/b \rceil} \log {b\choose c_i} + \lceil n/b \rceil =$$
$$=\log\prod_{i=1}^{\lceil n/b \rceil} \log {b\choose c_i} + \lceil n/b \rceil \leq $$
$$\log{n\choose \#_1} +  \lceil n/b \rceil \leq nH_o(B) +  \lceil n/b \rceil$$.

$T_d$ is storing $2^b$ entries and each entry is using $b$ bits of storage.
The total space used is then $nH_0(B) +  \lceil n/b \rceil + \lceil \log(b+1) \rceil + b2^b$.

To speed up the process of accessing block, we will store the pointers into every
$k$-th element of $O$. Efectively dividing the process of locating $O[i]$ into two
parts. The first is to find the nearest pointer leading to element before $i$ and
then moving at most $k$ places. This additional structure of roughly $n/bk$ integers
takes the space $\mathcal{O}(\log(n)\cdot n/bk)$.

\section{Rank}
\label{section:rank}

We would like to achieve a fast rank, yet we want to only add sublinear amount of
space on top of the current access supporting bit vector. There are two straightforward
solutions we can begin with. The first is to add zero additional space. Every
time we are asked to query for $rank_c(i)$, we run through all blocks that are
before $i/b$-th. This solution is very slow but it does not need any aditional space.
Nice property is that we do not need to decode all the blocks as the number of ones
is stored as class and is easily accessible.
The second approach is to precompute rank up to every block. This enables us to be very
fast in answering rank query as it consists at most from one constant lookup and then
decoding of one block. Widely used solution consists of these two approaches being merged
into idea of solution based on superblocks. We will store the precomputed rank, but not
for every block. Instead we choose a constant $k$ and divide the bit-vector into nonoverlaping
parts consisting of $k$ successive blocks named superblocks. If the number of blocks in
the bit-vector is not dividible by $k$ we simply left the last superblock shorter. We
now compute the rank value only for the beginnings of superblocks.

\section{Select}

Important property of select method is that it works like an invers to rank. In a sense that

                $$rank_c(select_c(i)) = i$$.

Thanks to rank being nondecreasing function, it is possible to binary search for the result
of $select_c(j)$. This can be nicely combined with the superblock solution from the previous
section \ref{section:rank}. At first, we binary search for the solution in the samples of rank
on the beginnings of superblocks. When we identify the right superblock, where the answer is
located, we simply linearly scan for the solution.

\section{Practical considerations}

\subsection{Block size}

First and one of the most important parameters of RRR implementation is the block
size $b$ used. For small values of $b$ it is possible to store the whole table
$T$ that helps us encode and decode the block. For bigger $b$ it is not practical
and many times impossible to store huge helper table. On the other hand, the bigger
block size yields a better compression because of smaller per block overhead. This was
something that the authors of Fast, Small, Simple Rank/Select on Bitmaps tried to
achieve and came up with something called on the fly-decoding. This works by encoding
and decoding without the need of helper table. Instead, it relies on a bit by bit decoding
of block thus taking $\mathcal{O}(b)$ time insead of constant time with the use of table.

Idea is that we decode the block from the beginning. On every bit, we consider putting a
zero bit on this position and count $comb$ - number of all combinations how the block can
look in this scenario. If the sequence number of the block - $o$ is bigger than this number,
we know that the decoded block will have one bit on this place. We proceed by subtracting
number $comb$ from $o$ and proceed sequentionally further.

This approach has some advantages and disadvantages. The biggest advantage is that it allows
us to use bigger block size $b$. The disadvantage is that it takes $\mathcal{O}(b)$ steps to decode the
block. Furthermore, the decoding contains branching and it is hard to parallelize the steps
in some meaningful way. Note that decoding is faster if we want to access the element
close to the beginning of the block as we can stop the process when we obtain this bit.