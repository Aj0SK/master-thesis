\chapter{Binary sequence representation}
\label{kap:kap2}

This chapter is dedicated to the previous work on the implementation of the bit vector, data
structure that stores binary sequence supporting methods access, rank and select. As we have
shown in Section~\ref{section:WaweletTree}, wavelet tree can be used to build a vector
for the general alphabet from the bit vector implementation. This is, why it is of utmost
importance to dedicate time to efficient and fast bit vector implementation. In this
chapter, we shall describe the currently available implementations. Let us start with
the succinct representation of a bit vector, in which we simply store bits one after another.
Then we shall look at the compressed representation.

\section{Bit vector implementation}

\subsection{Rank}
\label{section:rank}

Regarding rank, we are concerned in fact with two different methods namely $\rank_0(i)$ and
$\rank_1(i)$. It is easy to observe that following equation holds:

                    $$\rank_1(i) = i - \rank_0(i)$$

so it is enough to provide only the implementation of $\rank_1(i)$.
There are two straightforward solutions we can begin with to support rank on a bit vector
$B$. The first is to store no additional information. Every time we are asked
for $\rank_1(i)$, we run through all bits preceding $i$-th. This solution is not
practical for longer bit vectors but it does not add any additional space. The second
approach is to precompute rank of very bit. This enables us to answer rank in constant
time -- a single table lookup. However, the space needed to support this
solution is $(|B|\cdot\log |B|)$ bits. These two solutions can be combined together to obtain an
idea of a more practical solution. We will store the pre-computed rank, but not for every
bit. At first, we choose a constant $k$ and split bit vector into non-overlaping subsequences
of length $k$. We shall call one this subsequence \textit{superblock}. We then precompute rank
only for beginning of every superblock. This enables answering rank in time $\BigO(k)$ and
uses $\BigO((n/k)\log n)$ bits of memory.

Although this solution works well in practice, it is possible to answer rank in constant time
with sublinear space overhead. We start as in previous solution with setting length of the
superblock $k$ to $k=\log^2 n$ and precomputing rank for every $k$-th bit or as we call it for
every beginning of superblock. Every superblock we further divide into parts of size
$\frac{\log n}{2}$. We precompute the rank for all of these small blocks. To save space we only
precompute the rank from the beginning of the superblock. Now, when asked about the $\rank_1$ of
some position, we know in which superblock this number is located, so we know what is the number
of ones up to the superblock where this position is located. We also precomputed what is the
number of ones from the beginning of the superblock up to this block. The last think is to
find out the number of ones that are before this index in this superblock. This would be easy
to do linearly in time $\BigO(\log n)$. Hovewer, we are aiming for constant time so we need
also to precompute every possible rank query inside of the block. Thanks to block being short
there are not that many possible ways what the block can be, and what is the position in this
block we want to compute rank for. We shall now look at the space overhead that this solution
adds. The space overhead consists of three parts. The first part is precomputed rank for
every superblock. Number of superblocks is $\BigO(\frac{n}{\log^2 n})$ and we need
$\BigO(\log n)$ of space to store the rank so the total amount is

\begin{equation}
    \BigO(\frac{n}{\log^2 n}\cdot \log n).
    \label{eq:rank_space_1}
\end{equation}

Number of smaller blocks is $\BigO(\frac{n}{\frac{\log n}{2}})$ but now, every block stores only rank
from the beginning of superblock so this number is of size at most $\BigO(\log n)$ so we need
at most $\BigO(\log\log n)$ of bits to store it. The total amount of space for the smaller
blocks is then

\begin{equation}
    \BigO(\frac{n}{\lg n}\cdot \log\log n).
    \label{eq:rank_space_2}
\end{equation}

The third part is the precomputed table where we store for every possible block and every
possible rank query over it the result of the rank query. The number of different ways for
what the block can be is $2^{k} = 2^{\frac{\log n}{2}} = \sqrt{n}$. Number of possible rank queries
over it is $\BigO(\log n)$ so this makes the table with the total size 

\begin{equation}
    \BigO(\sqrt{n}\log n\log\log n)
    \label{eq:rank_space_3}
\end{equation}

as every entry is of size $\BigO(\log\log n)$. The total space used for this solution of rank
is therefore sum of \ref{eq:rank_space_1}, \ref{eq:rank_space_2} and \ref{eq:rank_space_3}.
As every one of these is using sublinear number of bits we can see that the solution uses just
sublinear space.

% TODO: ... depends on model... can be O(k/w) if we consider RAM model with w-bit registers
% that supports popcount
% TODO: teoria, succinct rank, dvojurovnove riesenie

\subsection{Select}
\label{section:select}

It is important that the select method is working much like an inverse to rank. This is given
by the fact that

                $$\rank_c(\select_c(i)) = i$$.

Thanks to rank being a nondecreasing function, it is possible to binary search for the result
of $\select_c(i)$. This can be nicely combined with the solution from the previous
section~\ref{section:rank}. At first, we binary search for the solution in the samples of rank
on the beginnings of every $k$-th bit. When we identify the correct block of $k$ bits, where
the answer is located, we simply linearly scan for the solution. This solution is not asking
for any additional memory on top of the space used for rank. The answer can be given in time
$\BigO(\log(n/k)+k)$.

% TODO: sem odcitovat zname teoreticke vysledky pre select

\section{Compressed representation}
\label{section:compressed_bv}

The main idea of \textit{RRR} is to split the bit sequence into blocks of constant length $b$.
This length is a parameter of the algorithm. After this, instead
of storing a single block as a bit sequence, we store each block encoded as a pair
of numbers $(c, o)$. Here, $c$ labels the class of the block and $o$ an offset into
the sequence of all blocks in this particular class. Most of the time, the $c$ is equal
to the number of ones in the block. Offset, on the other hand, is the index of block in
the sequence of all the blocks with $c$ ones, lexicographically sorted. Note that
class $c$ has a size ${b\choose c}$. The offset is then be bounded by:

				$$0 \leq o < {b\choose c}$$.

The process of obtaining $c$ and $o$ from the raw representation of block is called
\textit{encoding}. The opposite process is called \textit{decoding}. Although we would
like both processes to be fast. We do the encoding just once, at the initial construction
of the bit vector. On the other hand, decoding is done any time we are accessing a particular
bit in $B$ so it is more important for us to optimise so that decoding is as fast as possible.

For smaller block sizes, such as $b\leq 15$, it is in most cases reasonable to
generate two helper tables. One is named $E$ used for the encoding, where we can
index using the block casted to a number and get this particular block's offset.
The class can be computed trivially by counting a number of ones. The other table
named $D$ is two dimensional. In position $D[c][o]$ we store the block that
is associated with $(c, o)$. In other words, this is a block containing $c$ ones
and at the same time being $o$-th in the sequence of all lexicographically sorted
blocks containing $c$ ones. Both these tables $E$ and $D$ can be computed
by generating all the possible blocks in lexicographical order before we even
obtain the sequence that we will want to represent. After this precomputation,
the encoding and decoding of a block takes constant time.

For small values of $b$ it is possible to store the whole table $T$ that helps us
encode and decode the block. For bigger $b$ it is not practical and many times
impossible to store a huge helper table. On the other hand, the bigger block size
yields a better compression because of the smaller per block overhead. This was
something that \cite{navarro2012fast} tried to achieve and came up with the method
we shall call \textit{on the fly-decoding}. This works by encoding and decoding without
the need for the helper table. Instead, it relies on a bit by bit decoding of the block
thus taking $\BigO(b)$ time instead of constant time with the use of table.

Idea is that we decode the block from the beginning, bit by bit until the end. On every bit,
we consider putting a zero bit on this position and count $comb$ -- the number of all
combinations how the block can look in this scenario. If the sequence number of the block --
$o$ is bigger than this number, we know that the decoded block will have one bit on this place.
We proceed by subtracting the number $comb$ from $o$ and proceed sequentially further.

After slicing the sequence $B$ into blocks, we need to find a way how to store pairs $(c, o)$.
We shall store all the classes and offsets in separate arrays $C$ and $O$. The array $C$ shall
be implemented as an array of fixed length elements with a length equal to
$\ceil{\log_2(b)}$. The array $O$ shall be on the other hand implemented as an array of
variable length elements with the size needed to store $i$-th element $O[i]$ being
$\ceil{\log_2{b\choose C[i]}}$.

To provide access to the concrete bit, we need to first find out at which block this bit is located.
Let this be $i$-th block. To decode the block we at first need to get corresponding $C[i]$ and
$O[i]$ and then we can use a pre-computed table $D$ that matches $(c_i, o_i)$ to the original
block. Obtaining $C[i]$ is trivial as it is at a fixed offset in memory. Getting the value of
$O[i]$ is harder as it is not at a known offset but generally on an offset that can be computed
as

                $$\sum_{j=1}^{i-1} \ceil{\log_2{b\choose C[i]}}$$

but is not known previously. To compute it without any precomputed information,
we need to basically one by one skip over elements that come before $O[i]$.
Note that to compute the previous sum, we need only successive information from $C$ so these
accesses are quite cache-friendly and fast. However, to access the $i$-th block, we need in the
worst case to look at all the elements of $C$ and this takes $\BigO(n/b)$ time. To support
this version of access, we need to store the arrays $C, O$ and helper table $D$. We shall
now argue about the size of these structures. The size of these structures is
as follows:

$C$ is array of $\ceil{n/b}$ elements of fixed size $\ceil{\log(b+1)}$.

For array $O$ we argue that its size is bounded by

\begin{align*}
    \sum_{i=1}^{n/b} \ceil{{\log b\choose c_i}}
    &\leq \sum_{i=1}^{\ceil{n/b}} \log {b\choose c_i} + \ceil{n/b} \\
    &= \log\prod_{i=1}^{\ceil{n/b}} {b\choose c_i} + \ceil{n/b} \\
    &\leq \log{n\choose \#_1(B)} + \ceil{n/b} &\leq nH_0(B) + \ceil{n/b}
\end{align*}

where $\#_1(B)$ denotes the total number of ones in $B$. We obtained (someref) using the
observation that ${n\choose k} {m\choose \ell} \leq {n+m\choose k+\ell}$. This can be seen
when we interpret the left side of the equation as a number of ways we can choose $k$ elements
from $n$ elements and $\ell$ elements from another $m$ elements. This is all contained on the
right side of the equation that definitely includes all these combinations.

$D$ is storing $2^b$ entries and each entry is using $b$ bits of storage.
The total space used is then

$$nH_0(B) + \ceil{n/b} + \ceil{n/b} \cdot \ceil{\log(b+1)} + b2^b$$.

To speed up the process of accessing block, we will help ourselves store the pointers
into every $k$-th element of $O$. Effectively dividing the process of locating $O[i]$ into two
parts. The first is to find the nearest pointer leading to element before $i$ and
then move at most $k$ places. This additional structure of roughly $\frac{n}{bk}$ integers
takes the space $\BigO(\log(n)\cdot \frac{n}{bk})$.

\begin{figure}
	\centerline{
		\includegraphics[width=0.9\textwidth, height=0.3\textheight]{images/rrr}
	}
	\caption[TODO]{RRR implementation. $B$ shows the original bit sequence cut to
    blocks. $C$ stores the class which is in this case number of ones in the block.
    $O$ uses variable number of bits per entry, in general $i$-th entry uses
    $\ceil{\log_2{b\choose C[i]}}$ bits and stores the lexicographical order
    of this block in the class $C[i]$. For $k=2$ we can see a helper array $P$
    storing bit offsets into every $k$-th element namely $0, 2, 4\ldots$
	}
	\label{obr:RRRFinal}
	% source at https://docs.google.com/drawings/d/1f1M7e-dZIiIZh1RdgqnmptF3xWZBKqjms3f_aQwVMhg/edit
\end{figure}

To obtain interesting practical results, setting $b=\frac{\log(n)}{2}$ we get the total space
$nH_0(B) + o(n)$. So we are storing only a sublinear amount of data on top of the compressed
data.

\section{Practical considerations}

\subsection{Block size}

One of the most important parameters of RRR implementation is the block
size $b$ used. A bigger block size is better because of lower per bit overhead. Not
all block sizes are However used in practice. Very often, we are only interested in
block sizes of the form $2^n-1$. This is because number of ones in block of size $2^n-1$
can be anywhere between $0$ and $2^n-1$ making this in total $2^n$ options. Then, storing
this number in the fixed buckets of size $\log_2(2^n)$ makes us use all the available space.
So in practice, the commonly used bucket sizes are 15, 31, 63 and 127. However, there are
other limiting factors that come into play. For $b=15$ the encoding and decoding table
occupy roughly 64kB of space each. This is because there are $2^{15}$ of entries each taking
2 bytes of storage. For $b=31$ these two tables would consume roughly $2^{31}\cdot 4$ bytes
of storage which amounts to roughly 8.5GB of space. This makes this approach impossible in
practice and forces us to use the on the fly decoding for $b>15$. The disadvantage
is that it takes $\BigO(b)$ steps to decode the block. Furthermore, the decoding
contains branching and it is hard to parallelize the steps in some meaningful way.
Note that decoding is faster if we want to access the element close to the beginning of
the block as we can stop the process when we obtain this bit. Overall, this makes the block
size a parameter that we can adjust to balance between better space efficiency and faster
runtime performance.

% Random stuff follows

\subsection{Alternatives?}

RRR is best to use on sequences with low entropy, where the frequency of zeroes/ones is
strongly shifted to one or the other side such as 5-20\% percent of all bits are the same.
Where RRR stands out to some of its alternatives is that it is also very competitive in
cases where the global frequencies are very similar, but the bit sequence has a lot of places
where locally zeroes/ones are over-represented.

\subsection{Implementation improvements}

There are several used implementation speedups and improvements. They impact the run time in a
significant way. The first trick, being one of the most important for the real
life performance is taking care of special case when the number of ones in the block is zero
or equal to the length of the block $b$. These blocks are often called \textit{trivial}. In this
case, no decoding is necessary and we can immediately tell what the result is only from looking
into the helper array $C$. This may seem like not that important change. However, in many places
where RRR is used, the frequency of trivial blocks is high.
