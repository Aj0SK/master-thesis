\chapter{Binary sequence representation}
\label{kap:kap2}

Basic building block of many succinct data structures is \textit{vector}. Data structure
used to represent the sequence of characters over some alphabet $\Sigma$. As we
have shown in the section \ref{section:WaweletTree}, Wavelet tree can be used to
build vector for general alphabet from the bit-vector specialization. This is, why
it is of utmost importance to dedicate time to efficient and fast bit-vector
implementation. There are more ways to represent binary sequence. One of the widely
used technique is called \textit{RRR}. Named by its inventors Rajeev Raman, Venkatesh Raman,
S. Srinivasa Rao that proposed this data structure\cite{raman2007succinct}. We will look
at the first working RRR proposal of bit-vector\cite{claude2008practical}. Then we
shall look at the subsequent improvements over it \cite{navarro2012fast}.

\section{Division to blocks and access}

The main idea of RRR is to split the bit sequence into blocks of some constant
length $b$. This length is a parameter of algorithm and we will look at the consequences
of different choices of $b$. Then instead of storing the bit sequence we store the
block encoded as a pair of numbers $(c, o)$ where $c$ labels the class of this
block and $o$ an offset into the sequence of all blocks in this particular class.
Most of the time, the $c$ is equal to the number of ones in the block. Offset on
the other hand, is the index of block in the sequence of all the blocks with $c$ number
of ones, lexicographically sorted. Note that class $c$ has size

                    $${b\choose c}$$.

The offset is then be bounded by:

					$$0 \leq o < {b\choose c}$$.

The process of obtaining $c$ and $o$ from block is called \textit{encoding}.
The opposite process is called \textit{decoding}. Although we would like both
processes to be fast. We do the encoding just once, at the initial construction
of bit-vector. On the other hand, decoding is done any time we are interested in
block's particular bit so it is more important for us to optimise so that decoding
is as fast as possible.

For smaller block sizes, such as $b\leq 15$, it was in most cases reasonable to
generate two helper tables. One named $T_e$ used for the encoding, where we can
index using the block casted as a number and get this particular block's offset.
The class can be computed trivially by counting number of ones. The other table
named $T_d$ is two dimensional. In position $T_d[c][o]$ we store the block that
is associated with $(c, o)$. In other words, this is block containing $c$ ones
and at the same time being $o$-th in the sequence of all lexicographically sorted
blocks containing $c$ ones. Both these tables $T_e$ and $T_d$ can be computed
by generating all the possible blocks in lexicographical order before we even
obtain the sequence that we will want to represent. After this precomputation,
the encoding and decoding of a block takes constant time.

After slicing the sequence $S$ to blocks, we need to find a way how to store
pairs $(c, o)$. It is reasonable to store all the classes and offsets in separate
arrays $C$ and $O$. The array $C$ is usually implemented as a array of fixed length
elements of length $\lceil \log_2(b)\rceil$. The array $O$ is implemented as an
array of variable length elements and the size needed to store $i$-th element $O[i]$ is
$\log_2({b\choose C[i]})$. To decode the $i$-th block, we at first need to get
$C[i]$ and $O[i]$ and then we can use a precomputed table $T$ that matches $(c_i, o_i)$
to the original block. Obtaining $C[i]$ is trivial as it is at a fixed offset in
memory. Getting value of $O[i]$ is harder as it's generally on a index that can
be expressed as

                    $$\sum_{j=1}^{i-1} \log_2{b\choose C[i]}$$.

We need basically to one by one skip elements before $O[i]$ that have a variable
length. To speed up this process, we will store the pointers into every $k$-th
element of $O$ efectively dividing the process of locating $O[i]$ into two parts.
The first is to find the nearest pointer leading to element before $i$ and then
moving at most $k$ places. Note that to move in $O$ we need only successive
information from $C$ so these are very cache friendly. To support the access in
time $O(k)$ we need to store the arrays $C, O$, helper table for decoding $T$.
We shall now argue about the size of these structures. The size of these structures
are as follows:

$C$ is array of $\lceil n/b \rceil$ elements of fixed size $\lceil \log(b+1) \rceil$.

For array $O$ we argue that its size is bounded by

$$\sum_{i=1}^{n/b} \bigg\lceil{\log b\choose c_i}\bigg\rceil =
\sum_{i=1}^{\lceil n/b \rceil} \log {b\choose c_i} + \lceil n/b \rceil =$$
$$=\log\prod_{i=1}^{\lceil n/b \rceil} \log {b\choose c_i} + \lceil n/b \rceil \leq $$
$$\log{n\choose \#_1} +  \lceil n/b \rceil \leq nH_o(B) +  \lceil n/b \rceil$$.

$T$ can be imagined as numerous tables $T_c$ each with ${b\choose c}$ entries.
Number of  all of the entries is $2^b$ and each entry is using $b$ bits of storage.
The total space used is then $nH_o(B) +  \lceil n/b \rceil + \lceil \log(b+1) \rceil + b2^b$.

\section{Rank}

We would like to achieve a fast rank, yet adding only sublinear number of space on top of the
current access supporting bit vector. There are two extremes we can begin with. The first is to
add zero additional space and every time we are asked to query for $rank_c(i)$. We run through
all blocks that are before $i/b$-th. Nice property is that we do not need to decode all the blocks
as the number of ones is stored as class and is easily accessible.
Another extreme is to store rank up to that point for every block. This is fast as answering rank
query consists at most from one constant lookup and then decoding of one block.
Very good solution, that is used in practice is to join these two extreme ideas. We will store the
precomputed rank, but not for every block. Instead we choose a constant $k$ and divide
the bit-vector into nonoverlaping superblocks consisting of $k$ successive blocks. If the number of
blocks in the bit-vector is not dividible by $k$ we simply left the last superblock shorter.
We now compute the rank value only for the blocks that are on beginning of some superblock.

\section{Practical considerations}

\subsection{Block size}

First and one of the most important parameters of RRR implementation is the block
size $b$ used. For small values of $b$ it is possible to store the whole table
$T$ that helps us encode and decode the block. For bigger $b$ it is not practical
and many times impossible to store huge helper table. On the other hand, the bigger
block size yields a better compression because of smaller per block overhead. This was
something that the authors of Fast, Small, Simple Rank/Select on Bitmaps tried to
achieve and came up with something called on the fly-decoding. This works by encoding
and decoding without the need of helper table. Instead, it relies on a bit by bit decoding
of block thus taking $\mathcal{O}(b)$ time insead of constant time with the use of table.

Idea is that we decode the block from the beginning. On every bit, we consider putting a
zero bit on this position and count $comb$ - number of all combinations how the block can
look in this scenario. If the sequence number of the block - $o$ is bigger than this number,
we know that the decoded block will have one bit on this place. We proceed by subtracting
number $comb$ from $o$ and proceed sequentionally further.

This approach has some advantages and disadvantages. The biggest advantage is that it allows
us to use bigger block size $b$. The disadvantage is that it takes $O(b)$ steps to decode the
block. Furthermore, the decoding contains branching and it is hard to parallelize the steps
in some meaningful way. Note that decoding is faster if we want to access the element
close to the beginning of the block as we can stop the process when we obtain this bit.